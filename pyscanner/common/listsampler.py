"""LIST SAMPLER CONTAINER MODULE
This module contains functions for running pysusy in list sampling mode,
i.e. scanning points as directed by a list. It requires an appropriate
parameter generator setup module containing run options to be supplied 
to pysusy"""

from numpy import array, loadtxt, zeros, savetxt, shape, int_
import os

def listwrapper(inputfile,loglike,ndims,npar,coldict,parorder,mpicomm,root,verbose=False):
    """The list sampling driver function. This function performs the 
    job of a parameter generator, except no search algorithms are used.
    A list of points to be scanned is supplied and the function just 
    runs through them, calculating the likelihoods and observables 
    exactly the same as if, for example, multinest had been run
    
    Arguments:
    loglike - the function which calculates the loglikelihood value for
        the selected model point. Returns filled 'cube' and likelihood value of
        selected model point.
        
        Arguments that will be fed into loglike:
        npar - number of entries in 'cube' vector (which will be fed into
            loglike with the parameter values for the point to be computed, and 
            returned with the sample plus the values of the calculated observables
        ndims - number of parameters with variable values (the rest should be set
            in the template input file for the first program run)
            
    coldict - A dictionary specifying the relationship between the scan parameters
        and the columns of the input file they are recorded in. e.g {'M0' : 3, 'M12' : 4} etc.
    setupobjects - The core pysusy dictionary containing all the objects generated by
        the setup modules.
    mpicomm - The mpi4py MPI.COMM_WORLD object. Need this to synchronise processes.
    root - the location and base name to give output files, contains the unique
        job name. e.g. 'chains/testjob-1' (will have a number as allocated by pysusy internal jobID system.
        We will add another number here to indicate how the job is split up, for use with external systems
        for submitting array jobs to cluster computers (e.g. as on the Monash Sun Grid)).
        In pysusy.py this variable has the name 'outputpathbasename'.
    """
    
    collist= array(coldict.values())   #generate the list of columns to be extracted from the input file using the user supplied dictionary
    #WARNING!!! MAKE SURE TO CUT OUT JUST THE PARAMETER COLUMNS! IF THE INPUT FILE IS HUGE THIS
    #IS PROBABLY GOING TO CRASH
    print 'Reading input list {0}...'.format(inputfile)
    data = loadtxt(inputfile, usecols=(collist-1)) #get the data from the input file. The columns will match the order they were pulled from the dictionary, not the order they appear in the file. Subtract 1 from indices, expecting user to start numbering at 1, not 0 like python does.
    #if len(data.shape)==1: #if input has only one row it won't be a 2d array so we need to make it one
    #    data = [data];
    #data = data[0:163]  #TEMPORARY. TESTING ONLY.
    #Need to reorder the data columns so they match the order specified in the 'parorder' from setupobjects.
    #Create a dictionary from 'parorder' to do this mapping:
    orderdict={}    #create the dictionary
    for i,param in enumerate(parorder):
        orderdict[param] = i
    ordering = [orderdict[param] for param in coldict.keys()] #gets the names of the columns of 'data', in the same order as 'collist', then extracts their new ordering from 'orderdict'
    #we now have a list specifying the final column ordering, i.e. [3,2,0,1] says column 0 goes to column 3, 1 goes to 2 etc.
    #we want to transform this into a list of array indices which pick the columns out of data in the right order, i.e. for the above example
    #this list needs to be [2,3,1,0], i.e. take column 3 then 4 then 2 then 1 to make the new data set, i.e. we want
    #to know the list indices of the entries of 'ordering' in increasing sequence.
    orderindices = int_(zeros(len(ordering)))
    for i,val in enumerate(ordering):
        orderindices[val] = i   #basically we swap the indices and values
        
    #Calculate which rows of data this process is going to compute, using the info from 'jobid'
    n,size = mpicomm.rank, mpicomm.size #job 'n' of 'size'. Get this information from MPI.COMM_WORLD
    rows = range(n*len(data)/size,(n+1)*len(data)/size)   #start from n because job id numbers start at 0 from the sources I am using.
    
    #reorder the columns of data so that they match the order requested by 'parorder'
    #and extract the rows to be analysed by this process
    if verbose: print rows
    if verbose: print ordering
    if verbose: print orderindices
    
    data = data[rows,:][:,orderindices]     #extracts the rows, then the columns. Can't do both at once, at least not with this method; numpy has other ideas about how data[rows,ordering] should be interpreted.
    
    #Final setup:
    cube = zeros(npar)  #Initialise 'cube'. Must be npar long to be compatible with the 'loglike' function
    context = 'skipprior'  #this tells loglike not to perform any scaling on the values we feed it via 'cube'
    bufferlen=100    #This is the buffer length. We will output results to file this many iterations
    outdata = zeros([bufferlen,npar+2]) #I am using ndims+2 because multinest stores the posterior mass and -loglikelihood value for each point in slots 1 and 2. We will duplicate this format but leave the posterior mass slots empty. This makes the .info file match properly.    
    if verbose: print "Length checks:"
    if verbose: print "outdata", outdata.shape
    if verbose: print "cube:", len(cube)
    if verbose: print "ndims:", ndims
    if verbose: print "data[1,:]:", len(data[1,:])

    #Set name for full output file (for this process)
    outfilename = '{0}-{1}of{2}-.txt'.format(root,n,size)
    os.system('rm {0}'.format(outfilename))
    #Delete this file if it exists already (we will be catting onto it)
    #bjfN>IN FUTURE WILL NEED TO CHANGE HOW THIS WORKS TO IMPLEMENT RESUME
    #FUNCTIONALITY.
    #set name to give temporary chunk output file
    chunkfilename = '{0}-{1}of{2}-CHUNK-.txt'.format(root,n,size)
    print 'Storing output in {0}'.format(outfilename)
    #-----------Begin main loop----------#
    bi=0  #initialise buffer index
    for i in range(len(data)):
        cube[range(ndims)] = data[i,:]   #take the i'th row of 'data' and stick it into the first lot of entries in 'cube'. Should be in the right order by this stage.
        cube, lnew = loglike(cube,ndims,npar,context)   #run loglike, get back cube (now filled, i.e. with observables values and seperate loglikelihood contributions. Should be detailed in .info file)
        #store results in output array
        outdata[bi,1] = -2*lnew         #turn the log likelihood value into a chi**2 value before output
        outdata[bi,2:npar+2] = cube     #store cube in the output
        #OUTPUT TO (TEMPORARY) FILE EVERY 'bufferlen' ITERATIONS 
        if bi==bufferlen-1:
            savetxt(chunkfilename,outdata, fmt='%+1.8E')
            #cat this chunk onto the rest of the output
            os.system('cat {0} >> {1}'.format(chunkfilename,outfilename))
            #reset the buffer
            outdata = zeros([bufferlen,npar+2])
            bi=0
            continue    #restart the loop so bi does not get incremented
        bi+=1
    #deal with the rows still left in the buffer
    outdata=outdata[outdata.any(1)] #removes any rows containing only zeros
    savetxt(chunkfilename,outdata, fmt='%+1.8E')
    #cat this chunk onto the rest of the output
    os.system('cat {0} >> {1}'.format(chunkfilename,outfilename))
    
    #----------End main loop-------------#
    
    print 'Process {0} of {1} complete, waiting for other processes to finish to combine output...'.format(n,size)
    mpicomm.Barrier()  #MPI command for all processes to wait here until all processes catch up.
                    #Once we pass this, we know that all output files have been created.
    
    #-----OUTPUT GENERATION------#
    # Once all the processes have produced their pieces of output, we need
    # to stitch them all back together
    
    if n==0:   #if this is process zero
        finaloutfile='{0}.txt'.format(root,n,size)
        #delete this file if it exists already (so we can just append to it)
        os.system('rm {0}'.format(finaloutfile))
        print 'Combining output into {0}'.format(finaloutfile)
        for rank in range(size):
            outfilename2 = '{0}-{1}of{2}-.txt'.format(root,rank,size)
            os.system('cat {0} >> {1}'.format(outfilename2, finaloutfile))

    #-----------Done!------------#
